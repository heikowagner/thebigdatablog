{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Passwords/Common-Credentials/10-million-password-list-top-1000000.txt\"\n",
    "\n",
    "df_in = pd.read_csv(url, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_in = pd.read_csv(url, header=None)def add_space(x):\n",
    "    return \" \".join(x)\n",
    "\n",
    "df = df_in[0].dropna().apply(lambda x: add_space(x) + \"~~\")# apply(lambda x: \" \".join(letter for letter in x) + \"~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               1 2 3 4 5 6~~\n",
       "1           p a s s w o r d~~\n",
       "2           1 2 3 4 5 6 7 8~~\n",
       "3               q w e r t y~~\n",
       "4         1 2 3 4 5 6 7 8 9~~\n",
       "                 ...         \n",
       "999993          v j h t 0 8~~\n",
       "999994      V j h t 0 4 0 9~~\n",
       "999995          v j h t 0 4~~\n",
       "999996          v j h t 0 1~~\n",
       "999997        v j h t 0 0 8~~\n",
       "Name: 0, Length: 999996, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "import tensorflow as tf \n",
    "\n",
    "# Create a Tokenizer object\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Fit the Tokenizer on the sentences\n",
    "tokenizer.fit_on_texts(df)\n",
    "\n",
    "# Convert the sentences to tokens\n",
    "tokens = tokenizer.texts_to_sequences(df)\n",
    "\n",
    "# Pad the tokens to the same length\n",
    "padded_tokens = pad_sequences(tokens, padding=\"post\")\n",
    "\n",
    "# Generate a training dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(padded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  8, 14, ...,  0,  0,  0],\n",
       "       [31,  1, 10, ...,  0,  0,  0],\n",
       "       [ 2,  8, 14, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [32, 35, 25, ...,  0,  0,  0],\n",
       "       [32, 35, 25, ...,  0,  0,  0],\n",
       "       [32, 35, 25, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer_pt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\HEIKOW~1\\AppData\\Local\\Temp/ipykernel_21804/2172426256.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Size of input vocab plus start and end tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0minput_vocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer_pt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mtarget_vocab_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer_en\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer_pt' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "d_model = 512\n",
    "dff=2048\n",
    "maximum_position_encoding = 10000\n",
    "\n",
    "# Size of input vocab plus start and end tokens\n",
    "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "\n",
    "# Encoder ##################################\n",
    "input = tf.keras.layers.Input(shape=(None,))\n",
    "x = tf.keras.layers.Embedding(input_vocab_size, d_model)(input)\n",
    "\n",
    "## positional encoding\n",
    "scaling_factor = tf.keras.backend.constant(np.sqrt(d_model), shape = (1,1,1))\n",
    "x = tf.keras.layers.Multiply()([x,scaling_factor])\n",
    "pos = positional_encoding(maximum_position_encoding, d_model)\n",
    "x = tf.keras.layers.Add()([x, pos[: , :tf.shape(x)[1], :]] )\n",
    "\n",
    "## self-attention\n",
    "query = tf.keras.layers.Dense(d_model)(x)\n",
    "value = tf.keras.layers.Dense(d_model)(x)\n",
    "key = tf.keras.layers.Dense(d_model)(x)\n",
    "attention = tf.keras.layers.Attention()([query, value, key])\n",
    "attention = tf.keras.layers.Dense(d_model)(attention)\n",
    "x = tf.keras.layers.Add()([x , attention]) # residual connection\n",
    "x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "## Feed Forward\n",
    "dense = tf.keras.layers.Dense(dff, activation='relu')(x)\n",
    "dense = tf.keras.layers.Dense(d_model)(dense)\n",
    "x = tf.keras.layers.Add()([x , dense])     # residual connection\n",
    "encoder = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "\n",
    "# Decoder ##################################\n",
    "target = tf.keras.layers.Input(shape=(None,))\n",
    "x = tf.keras.layers.Embedding(target_vocab_size, d_model)(target)\n",
    "\n",
    "## positional encoding\n",
    "x = tf.keras.layers.Multiply()([x,scaling_factor])\n",
    "pos = positional_encoding(maximum_position_encoding, d_model)\n",
    "x = tf.keras.layers.Add()([x, pos[: , :tf.shape(x)[1], :] ])           \n",
    "\n",
    "## self-attention\n",
    "query = tf.keras.layers.Dense(d_model)(x)\n",
    "value = tf.keras.layers.Dense(d_model)(x)\n",
    "key = tf.keras.layers.Dense(d_model)(x)\n",
    "attention = tf.keras.layers.Attention(causal = True)([query, value, key])\n",
    "attention = tf.keras.layers.Dense(d_model)(attention)\n",
    "x = tf.keras.layers.Add()([x , attention])  # residual connection\n",
    "x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "## encoder-decoder attention\n",
    "query = tf.keras.layers.Dense(d_model)(x)\n",
    "value = tf.keras.layers.Dense(d_model)(encoder)\n",
    "key = tf.keras.layers.Dense(d_model)(encoder)\n",
    "attention = tf.keras.layers.Attention()([query, value, key])\n",
    "attention = tf.keras.layers.Dense(d_model)(attention)\n",
    "x = tf.keras.layers.Add()([x , attention])  # residual connection\n",
    "x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "## Feed Forward\n",
    "dense = tf.keras.layers.Dense(dff, activation='relu')(x)\n",
    "dense = tf.keras.layers.Dense(d_model)(dense)\n",
    "x = tf.keras.layers.Add()([x , dense])      # residual connection\n",
    "decoder = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "######################################################\n",
    "\n",
    "x = tf.keras.layers.Dense(target_vocab_size)(decoder)\n",
    "base_model = tf.keras.models.Model(inputs=[input,target], outputs=x)\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.fit(x = [input, target[:, :-1]], y = target[:, 1:], epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [tokenizer_en.vocab_size]for _ in range(40):\n",
    "   predict = model.predict([input ,np.asarray([result])])\n",
    "   result.append(np.argmax(predict[-1,-1]))\n",
    "   if result[-1] == tokenizer_en.vocab_size + 1:\n",
    "       break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "query = tf.keras.layers.Input(shape=(None,3,))\n",
    "value = tf.keras.layers.Input(shape=(4,2,)) \n",
    "key = tf.keras.layers.Input(shape=(4,3,))\n",
    "\n",
    "x = tf.keras.layers.Attention()([query, value, key])\n",
    "model = tf.keras.models.Model(inputs=[query, value, key], outputs=x)\n",
    "\n",
    "temp_k = tf.constant([[[10,0,0],\n",
    "                       [0,10,0],\n",
    "                       [0,0,10],\n",
    "                       [0,0,10]]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[[   1,0],\n",
    "                       [  10,0],\n",
    "                       [ 100,5],\n",
    "                       [1000,6]]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "temp_q = tf.constant([[[0, 10, 0]]], dtype=tf.float32)  # (1, 3)\n",
    "\n",
    "preds = model.predict([temp_q,temp_v,temp_k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19d1d53a962d236aa061289c2ac16dc8e6d9648c89fe79f459ae9a3493bc67b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
