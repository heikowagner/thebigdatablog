{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-26 11:53:36.629259: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-02-26 11:53:36.629743: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-02-26 11:53:36.630024: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-02-26 11:53:36.630817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-02-26 11:53:36.630848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-02-26 11:53:36.631157: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-02-26 11:53:36.631209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 3920 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 SUPER, pci bus id: 0000:0a:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras.utils as ku \n",
    "import pickle\n",
    "import logging\n",
    "\n",
    "from generators import PasswordBatchGenerator\n",
    "from data_preparation import construct_subsequences, ff_labels, rnn_labels, add_spaces, transform_password, pad_and_tokenize\n",
    "from predict_utils import predict_letter, predict_next_letter, predict_password\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "print(tf.keras.__version__)\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('data.pkl', 'rb') as file:\n",
    "        df = pickle.load(file)\n",
    "except:\n",
    "    # Load the Password list into Memory\n",
    "    url = \"https://raw.githubusercontent.com/danielmiessler/SecLists/master/Passwords/Common-Credentials/10-million-password-list-top-1000000.txt\"\n",
    "\n",
    "    df_in = pd.read_csv(url, header=None)\n",
    "\n",
    "    df = transform_password(df_in)\n",
    "    with open('data.pkl', 'wb') as file:\n",
    "        # Cache Data\n",
    "        pickle.dump(df, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 10_000 # len(df)\n",
    "sequences = construct_subsequences(df, limit=limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27523/168904925.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  letter_case = np.array( [ [np.int32(l.isupper()+1) for l in letters] for letters in sequences] )\n"
     ]
    }
   ],
   "source": [
    "letter_case = np.array( [ [np.int32(l.isupper()+1) for l in letters] for letters in sequences] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define two different tokenizer, one for capital letters and one for letters\n",
    "\n",
    "tokenizer_letter  = Tokenizer(filters=None, char_level=True, lower=True)\n",
    "tokenizer_letter.fit_on_texts(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer_case.texts_to_sequences(letter_case)\n",
    "\n",
    "max_length = max([len(x) for x in sequences])\n",
    "\n",
    "# Translation from Word to Token and back\n",
    "word2idx = tokenizer_letter.word_index\n",
    "idx2word = tokenizer_letter.index_word\n",
    "vocab_size = len(word2idx) + 1\n",
    "\n",
    "predictors_letter, label_letter = rnn_labels(pad_and_tokenize(sequences, tokenizer_letter, max_length), vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Binaries we do not need a tokenizer, we just pad\n",
    "predictors_case = np.array([ np.pad(x[0:-1], (max_length-len(x[0:-1])-1, 0), constant_values=0) for x in letter_case])\n",
    "label_case = np.array([x[-1] for x in letter_case])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(56508, 32), dtype=int32, numpy=\n",
       "array([[0, 0, 0, ..., 0, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 1, 1, 1]], dtype=int32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.layers.Concatenate(axis=1)([predictors_letter, predictors_case])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden, da „connect_qtconsole“ nicht von „unknown location“ importiert werden konnte.\n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>hier</a>, um weitere Informationen zu erhalten."
     ]
    }
   ],
   "source": [
    "# Init generator\n",
    "# We are not using a generator for now\n",
    "# batch_size= 10_000\n",
    "# training_batch = PasswordBatchGenerator(predictors, np.array(labels), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden, da „connect_qtconsole“ nicht von „unknown location“ importiert werden konnte.\n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>hier</a>, um weitere Informationen zu erhalten."
     ]
    }
   ],
   "source": [
    "try:\n",
    "  with open(f'/app/models/lstm_model_multivar_{str(limit)}.pkl', 'rb') as file:\n",
    "      lstm_model = pickle.load(file)\n",
    "      logging.info(lstm_model.summary())\n",
    "except:\n",
    "  lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 10, input_length=max_length-1), #Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "    tf.keras.layers.Masking(mask_value=0),\n",
    "    tf.keras.layers.LSTM(100), # skips masked timesteps\n",
    "    tf.keras.layers.Dropout(0.1),\n",
    "    tf.keras.layers.Dense(vocab_size+1, activation='softmax')\n",
    "  ])\n",
    "  lstm_model.compile(\n",
    "                #optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                optimizer='adam')\n",
    "\n",
    "  lstm_model.fit( training_batch , epochs=100, verbose=1)\n",
    "\n",
    "  # cache the model\n",
    "  with open(f'/app/models/lstm_model_{str(limit)}.pkl', 'wb') as file:\n",
    "    # A new file will be created\n",
    "    pickle.dump(lstm_model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden, da „connect_qtconsole“ nicht von „unknown location“ importiert werden konnte.\n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>hier</a>, um weitere Informationen zu erhalten."
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Dropout, LSTM\n",
    "\n",
    "# letter_inputs = tf.keras.layers.Embedding(vocab_size, 10, input_length=max_length-1) #\n",
    "letter_inputs = tf.keras.layers.Input(shape=max_length-1) # 7 past steps and variables\n",
    "case_inputs = tf.keras.layers.Input(shape=max_length-1) # 7 past steps and variables\n",
    "\n",
    "inputs = tf.keras.layers.concatenate([letter_inputs, case_inputs])\n",
    "\n",
    "# m = tf.keras.layers.Embedding(vocab_size, 10, input_length=max_length-1)(m)\n",
    "m = tf.keras.layers.Masking(mask_value=0)(inputs)\n",
    "m = tf.keras.layers.LSTM(100)(m)\n",
    "m = tf.keras.layers.Dropout(0.1)(m)\n",
    "outputA = tf.keras.layers.Dense(vocab_size, activation='softmax')(m)\n",
    "outputB = tf.keras.layers.Dense(1, activation='softmax')(m)\n",
    "\n",
    "m = tf.keras.Model(inputs=[inputs], outputs=[outputA, outputB])\n",
    "m.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "# m.fit(training_batch,  epochs=100, verbose=1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden, da „connect_qtconsole“ nicht von „unknown location“ importiert werden konnte.\n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>hier</a>, um weitere Informationen zu erhalten."
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/66845924/multi-input-multi-output-model-with-keras-functional-api\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "(xtrain, ytrain), (xtest, _) = keras.datasets.mnist.load_data()\n",
    "\n",
    "xtrain = xtrain[:10000] # both input sample should be same number \n",
    "ytrain = ytrain[:10000] # both input sample should be same number\n",
    "\n",
    "y_out_a = keras.utils.to_categorical(ytrain, num_classes=10)\n",
    "y_out_b = keras.utils.to_categorical((ytrain % 2 == 0).astype(int), num_classes=2)\n",
    "y_out_c = tf.square(tf.cast(ytrain, tf.float32))\n",
    "\n",
    "print(xtrain.shape, xtest.shape) \n",
    "print(y_out_a.shape, y_out_b.shape, y_out_c.shape)\n",
    "# (10000, 28, 28) (10000, 28, 28)\n",
    "# (10000, 10) (10000, 2) (10000,)\n",
    "\n",
    "# Next, we need to modify some parts of the above model to take multi-input. And next if you now plot, you will see the new graph.\n",
    "\n",
    "input0 = keras.Input(shape=(28, 28, 1), name=\"img2\")\n",
    "input1 = keras.Input(shape=(28, 28, 1), name=\"img1\")\n",
    "concate_input = keras.layers.Concatenate()([input0, input1])\n",
    "\n",
    "x = keras.layers.Conv2D(16, 3, activation=\"relu\")(concate_input)\n",
    "x = keras.layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = keras.layers.MaxPooling2D(3)(x)\n",
    "x = keras.layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = keras.layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
    "x = keras.layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "out_a = keras.layers.Dense(10, activation='softmax', name='10cls')(x)\n",
    "out_b = keras.layers.Dense(2, activation='softmax', name='2cls')(x)\n",
    "out_c = keras.layers.Dense(1, activation='linear', name='1rg')(x)\n",
    "\n",
    "\n",
    "# multi-input , multi-output\n",
    "encoder = keras.Model( inputs = [input0, input1], \n",
    "                       outputs = [out_a, out_b, out_c], name=\"encoder\")\n",
    "\n",
    "keras.utils.plot_model(\n",
    "    encoder\n",
    ")\n",
    "encoder.compile(\n",
    "    loss = {\n",
    "        \"10cls\": tf.keras.losses.CategoricalCrossentropy(),\n",
    "        \"2cls\": tf.keras.losses.CategoricalCrossentropy(),\n",
    "        \"1rg\": tf.keras.losses.MeanSquaredError()\n",
    "    },\n",
    "\n",
    "    metrics = {\n",
    "        \"10cls\": 'accuracy',\n",
    "        \"2cls\": 'accuracy',\n",
    "        \"1rg\": 'mse'\n",
    "    },\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    ")\n",
    "encoder.fit([xtrain, xtest], [y_out_a, y_out_b, y_out_c], \n",
    "             epochs=30, batch_size = 256, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden, da „connect_qtconsole“ nicht von „unknown location“ importiert werden konnte.\n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>hier</a>, um weitere Informationen zu erhalten."
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/66845924/multi-input-multi-output-model-with-keras-functional-api\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "(xtrain, ytrain), (xtest, _) = keras.datasets.mnist.load_data()\n",
    "\n",
    "xtrain = xtrain[:10000] # both input sample should be same number \n",
    "ytrain = ytrain[:10000] # both input sample should be same number\n",
    "\n",
    "y_out_a = keras.utils.to_categorical(ytrain, num_classes=10)\n",
    "y_out_b = keras.utils.to_categorical((ytrain % 2 == 0).astype(int), num_classes=2)\n",
    "y_out_c = tf.square(tf.cast(ytrain, tf.float32))\n",
    "\n",
    "print(xtrain.shape, xtest.shape) \n",
    "print(y_out_a.shape, y_out_b.shape, y_out_c.shape)\n",
    "# (10000, 28, 28) (10000, 28, 28)\n",
    "# (10000, 10) (10000, 2) (10000,)\n",
    "\n",
    "# Next, we need to modify some parts of the above model to take multi-input. And next if you now plot, you will see the new graph.\n",
    "\n",
    "input0 = keras.Input(shape=(28, 28, 1), name=\"img2\")\n",
    "input1 = keras.Input(shape=(28, 28, 1), name=\"img1\")\n",
    "concate_input = keras.layers.Concatenate()([input0, input1])\n",
    "\n",
    "x = keras.layers.Conv2D(16, 3, activation=\"relu\")(concate_input)\n",
    "x = keras.layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = keras.layers.MaxPooling2D(3)(x)\n",
    "x = keras.layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = keras.layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
    "x = keras.layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "out_a = keras.layers.Dense(10, activation='softmax', name='10cls')(x)\n",
    "out_b = keras.layers.Dense(2, activation='softmax', name='2cls')(x)\n",
    "out_c = keras.layers.Dense(1, activation='linear', name='1rg')(x)\n",
    "\n",
    "\n",
    "# multi-input , multi-output\n",
    "encoder = keras.Model( inputs = [input0, input1], \n",
    "                       outputs = [out_a, out_b, out_c], name=\"encoder\")\n",
    "\n",
    "keras.utils.plot_model(\n",
    "    encoder\n",
    ")\n",
    "encoder.compile(\n",
    "    loss = {\n",
    "        \"10cls\": tf.keras.losses.CategoricalCrossentropy(),\n",
    "        \"2cls\": tf.keras.losses.CategoricalCrossentropy(),\n",
    "        \"1rg\": tf.keras.losses.MeanSquaredError()\n",
    "    },\n",
    "\n",
    "    metrics = {\n",
    "        \"10cls\": 'accuracy',\n",
    "        \"2cls\": 'accuracy',\n",
    "        \"1rg\": 'mse'\n",
    "    },\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    ")\n",
    "encoder.fit([xtrain, xtest], [y_out_a, y_out_b, y_out_c], \n",
    "             epochs=30, batch_size = 256, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden, da „connect_qtconsole“ nicht von „unknown location“ importiert werden konnte.\n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>hier</a>, um weitere Informationen zu erhalten."
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(\n",
    "    encoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden, da „connect_qtconsole“ nicht von „unknown location“ importiert werden konnte.\n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>hier</a>, um weitere Informationen zu erhalten."
     ]
    }
   ],
   "source": [
    "arr = np.array( predictors )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden, da „connect_qtconsole“ nicht von „unknown location“ importiert werden konnte.\n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>hier</a>, um weitere Informationen zu erhalten."
     ]
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden, da „connect_qtconsole“ nicht von „unknown location“ importiert werden konnte.\n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>hier</a>, um weitere Informationen zu erhalten."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# pickle model\n",
    "with open('/root/model.pkl', 'wb') as file:\n",
    "    # A new file will be created\n",
    "    pickle.dump(lstm_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden, da „connect_qtconsole“ nicht von „unknown location“ importiert werden konnte.\n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>hier</a>, um weitere Informationen zu erhalten."
     ]
    }
   ],
   "source": [
    "predict_next_letter(\"lov\", ff_model, tokenizer, True)\n",
    "predict_next_letter(\"lov\", rnn_model, tokenizer, True)\n",
    "predict_next_letter(\"lov\", lstm_model, tokenizer, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden, da „connect_qtconsole“ nicht von „unknown location“ importiert werden konnte.\n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>hier</a>, um weitere Informationen zu erhalten."
     ]
    }
   ],
   "source": [
    "print( predict_password(\"lov\", ff_model, tokenizer) )\n",
    "print( predict_password(\"lov\", rnn_model, tokenizer) )\n",
    "print( predict_password(\"lov\", lstm_model, tokenizer) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mDer Kernel konnte nicht gestartet werden, da „connect_qtconsole“ nicht von „unknown location“ importiert werden konnte.\n",
      "\u001b[1;31mKlicken Sie <a href='https://aka.ms/kernelFailuresModuleImportErrFromFile'>hier</a>, um weitere Informationen zu erhalten."
     ]
    }
   ],
   "source": [
    "#https://jhui.github.io/2017/03/15/RNN-LSTM-GRU/\n",
    "#https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn\n",
    "#https://www.kaggle.com/code/namansood/nlp-guide-next-word-prediction-and-deep-learning\n",
    "#https://stackoverflow.com/questions/43341374/tensorflow-dynamic-rnn-lstm-how-to-format-input\n",
    "#https://r2rt.com/recurrent-neural-networks-in-tensorflow-i.html\n",
    "#https://medium.com/@shivambansal36/language-modelling-text-generation-using-lstms-deep-learning-for-nlp-ed36b224b275\n",
    "#https://ai.stackexchange.com/questions/18198/what-is-the-difference-between-lstm-and-rnn#:~:text=The%20main%20difference%20between%20an%20LSTM%20unit%20and,better%20the%20flow%20of%20information%20through%20the%20unit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
