 WARN [2020-01-28 18:21:10,085] ({main} ZeppelinConfiguration.java[create]:97) - Failed to load configuration, proceeding with a default
 INFO [2020-01-28 18:21:10,196] ({main} ZeppelinConfiguration.java[create]:109) - Server Host: 0.0.0.0
 INFO [2020-01-28 18:21:10,198] ({main} ZeppelinConfiguration.java[create]:111) - Server Port: 8080
 INFO [2020-01-28 18:21:10,198] ({main} ZeppelinConfiguration.java[create]:115) - Context Path: /
 INFO [2020-01-28 18:21:10,210] ({main} ZeppelinConfiguration.java[create]:116) - Zeppelin Version: 0.7.2
 INFO [2020-01-28 18:21:10,259] ({main} Log.java[initialized]:186) - Logging initialized @1095ms
 INFO [2020-01-28 18:21:10,434] ({main} ZeppelinServer.java[setupWebAppContext]:343) - ZeppelinServer Webapp path: /zeppelin/webapps
 INFO [2020-01-28 18:21:10,906] ({main} ZeppelinServer.java[main]:187) - Starting zeppelin server
 INFO [2020-01-28 18:21:10,910] ({main} Server.java[doStart]:327) - jetty-9.2.15.v20160210
 INFO [2020-01-28 18:21:16,125] ({main} StandardDescriptorProcessor.java[visitServlet]:297) - NO JSP Support for /, did not find org.eclipse.jetty.jsp.JettyJspServlet
 WARN [2020-01-28 18:21:16,476] ({main} Helium.java[loadConf]:101) - /zeppelin/conf/helium.json does not exists
 WARN [2020-01-28 18:21:17,754] ({main} Interpreter.java[register]:406) - Static initialization is deprecated for interpreter sql, You should change it to use interpreter-setting.json in your jar or interpreter/{interpreter}/interpreter-setting.json
 INFO [2020-01-28 18:21:17,757] ({main} InterpreterSettingManager.java[init]:305) - Interpreter psql.sql found. class=org.apache.zeppelin.postgresql.PostgreSqlInterpreter
 INFO [2020-01-28 18:21:18,652] ({main} InterpreterSettingManager.java[init]:337) - InterpreterSettingRef name ignite
 INFO [2020-01-28 18:21:18,654] ({main} InterpreterSettingManager.java[init]:337) - InterpreterSettingRef name python
 INFO [2020-01-28 18:21:18,655] ({main} InterpreterSettingManager.java[init]:337) - InterpreterSettingRef name jdbc
 INFO [2020-01-28 18:21:18,659] ({main} InterpreterSettingManager.java[init]:337) - InterpreterSettingRef name psql
 INFO [2020-01-28 18:21:18,661] ({main} InterpreterSettingManager.java[init]:337) - InterpreterSettingRef name lens
 INFO [2020-01-28 18:21:18,661] ({main} InterpreterSettingManager.java[init]:337) - InterpreterSettingRef name pig
 INFO [2020-01-28 18:21:18,662] ({main} InterpreterSettingManager.java[init]:337) - InterpreterSettingRef name flink
 INFO [2020-01-28 18:21:18,663] ({main} InterpreterSettingManager.java[init]:337) - InterpreterSettingRef name angular
 INFO [2020-01-28 18:21:18,663] ({main} InterpreterSettingManager.java[init]:337) - InterpreterSettingRef name livy
 INFO [2020-01-28 18:21:18,664] ({main} InterpreterSettingManager.java[init]:337) - InterpreterSettingRef name file
 INFO [2020-01-28 18:21:18,665] ({main} InterpreterSettingManager.java[init]:337) - InterpreterSettingRef name elasticsearch
 INFO [2020-01-28 18:21:18,665] ({main} InterpreterSettingManager.java[init]:337) - InterpreterSettingRef name spark
 INFO [2020-01-28 18:21:18,666] ({main} InterpreterSettingManager.java[init]:337) - InterpreterSettingRef name sh
 INFO [2020-01-28 18:21:18,666] ({main} InterpreterSettingManager.java[init]:337) - InterpreterSettingRef name cassandra
 INFO [2020-01-28 18:21:18,667] ({main} InterpreterSettingManager.java[init]:337) - InterpreterSettingRef name md
 INFO [2020-01-28 18:21:18,668] ({main} InterpreterSettingManager.java[init]:337) - InterpreterSettingRef name alluxio
 INFO [2020-01-28 18:21:18,668] ({main} InterpreterSettingManager.java[init]:337) - InterpreterSettingRef name bigquery
 INFO [2020-01-28 18:21:18,669] ({main} InterpreterSettingManager.java[init]:337) - InterpreterSettingRef name hbase
 INFO [2020-01-28 18:21:18,669] ({main} InterpreterSettingManager.java[init]:337) - InterpreterSettingRef name kylin
 INFO [2020-01-28 18:21:18,727] ({main} InterpreterSettingManager.java[init]:366) - InterpreterSetting group python : id=2EYXBRDJ5, name=python
 INFO [2020-01-28 18:21:18,728] ({main} InterpreterSettingManager.java[init]:366) - InterpreterSetting group angular : id=2EZB6UDVP, name=angular
 INFO [2020-01-28 18:21:18,758] ({main} InterpreterSettingManager.java[init]:366) - InterpreterSetting group flink : id=2F227ABNE, name=flink
 INFO [2020-01-28 18:21:18,759] ({main} InterpreterSettingManager.java[init]:366) - InterpreterSetting group ignite : id=2EYQJZP8M, name=ignite
 INFO [2020-01-28 18:21:18,760] ({main} InterpreterSettingManager.java[init]:366) - InterpreterSetting group kylin : id=2EYPZBKJE, name=kylin
 INFO [2020-01-28 18:21:18,761] ({main} InterpreterSettingManager.java[init]:366) - InterpreterSetting group bigquery : id=2EZS8KEA9, name=bigquery
 INFO [2020-01-28 18:21:18,762] ({main} InterpreterSettingManager.java[init]:366) - InterpreterSetting group hbase : id=2EYZVQXYX, name=hbase
 INFO [2020-01-28 18:21:18,763] ({main} InterpreterSettingManager.java[init]:366) - InterpreterSetting group lens : id=2EZVG6G57, name=lens
 INFO [2020-01-28 18:21:18,764] ({main} InterpreterSettingManager.java[init]:366) - InterpreterSetting group md : id=2EZP4C85J, name=md
 INFO [2020-01-28 18:21:18,764] ({main} InterpreterSettingManager.java[init]:366) - InterpreterSetting group alluxio : id=2EZWADT58, name=alluxio
 INFO [2020-01-28 18:21:18,765] ({main} InterpreterSettingManager.java[init]:366) - InterpreterSetting group psql : id=2F2PCVGFR, name=psql
 INFO [2020-01-28 18:21:18,765] ({main} InterpreterSettingManager.java[init]:366) - InterpreterSetting group livy : id=2EZ23WZFS, name=livy
 INFO [2020-01-28 18:21:18,766] ({main} InterpreterSettingManager.java[init]:366) - InterpreterSetting group cassandra : id=2F1JMZ845, name=cassandra
 INFO [2020-01-28 18:21:18,766] ({main} InterpreterSettingManager.java[init]:366) - InterpreterSetting group pig : id=2EZVF4BQW, name=pig
 INFO [2020-01-28 18:21:18,767] ({main} InterpreterSettingManager.java[init]:366) - InterpreterSetting group elasticsearch : id=2F24DHBF6, name=elasticsearch
 INFO [2020-01-28 18:21:18,767] ({main} InterpreterSettingManager.java[init]:366) - InterpreterSetting group spark : id=2EZR2722K, name=spark
 INFO [2020-01-28 18:21:18,768] ({main} InterpreterSettingManager.java[init]:366) - InterpreterSetting group sh : id=2EZS4MDFC, name=sh
 INFO [2020-01-28 18:21:18,769] ({main} InterpreterSettingManager.java[init]:366) - InterpreterSetting group file : id=2EZNTS3NA, name=file
 INFO [2020-01-28 18:21:18,770] ({main} InterpreterSettingManager.java[init]:366) - InterpreterSetting group jdbc : id=2EYQNFPAU, name=jdbc
 INFO [2020-01-28 18:21:18,783] ({main} InterpreterFactory.java[<init>]:130) - shiroEnabled: false
 INFO [2020-01-28 18:21:18,903] ({main} VfsLog.java[info]:138) - Using "/tmp/vfs_cache" as temporary files store.
 INFO [2020-01-28 18:21:19,009] ({main} GitNotebookRepo.java[<init>]:63) - Opening a git repo at '/zeppelin/notebook'
 INFO [2020-01-28 18:21:19,515] ({main} NotebookAuthorization.java[loadFromFile]:96) - /zeppelin/conf/notebook-authorization.json
 INFO [2020-01-28 18:21:19,517] ({main} Credentials.java[loadFromFile]:102) - /zeppelin/conf/credentials.json
 INFO [2020-01-28 18:21:19,600] ({main} StdSchedulerFactory.java[instantiate]:1184) - Using default implementation for ThreadExecutor
 INFO [2020-01-28 18:21:19,609] ({main} SimpleThreadPool.java[initialize]:268) - Job execution threads will use class loader of thread: main
 INFO [2020-01-28 18:21:19,649] ({main} SchedulerSignalerImpl.java[<init>]:61) - Initialized Scheduler Signaller of type: class org.quartz.core.SchedulerSignalerImpl
 INFO [2020-01-28 18:21:19,652] ({main} QuartzScheduler.java[<init>]:240) - Quartz Scheduler v.2.2.1 created.
 INFO [2020-01-28 18:21:19,654] ({main} RAMJobStore.java[initialize]:155) - RAMJobStore initialized.
 INFO [2020-01-28 18:21:19,656] ({main} QuartzScheduler.java[initialize]:305) - Scheduler meta-data: Quartz Scheduler (v2.2.1) 'DefaultQuartzScheduler' with instanceId 'NON_CLUSTERED'
  Scheduler class: 'org.quartz.core.QuartzScheduler' - running locally.
  NOT STARTED.
  Currently in standby mode.
  Number of jobs executed: 0
  Using thread pool 'org.quartz.simpl.SimpleThreadPool' - with 10 threads.
  Using job-store 'org.quartz.simpl.RAMJobStore' - which does not support persistence. and is not clustered.

 INFO [2020-01-28 18:21:19,657] ({main} StdSchedulerFactory.java[instantiate]:1339) - Quartz scheduler 'DefaultQuartzScheduler' initialized from default resource file in Quartz package: 'quartz.properties'
 INFO [2020-01-28 18:21:19,658] ({main} StdSchedulerFactory.java[instantiate]:1343) - Quartz scheduler version: 2.2.1
 INFO [2020-01-28 18:21:19,659] ({main} QuartzScheduler.java[start]:575) - Scheduler DefaultQuartzScheduler_$_NON_CLUSTERED started.
 INFO [2020-01-28 18:21:20,900] ({main} FolderView.java[createFolder]:107) - Create folder /
 INFO [2020-01-28 18:21:20,901] ({main} Folder.java[setParent]:168) - Set parent of / to /
 INFO [2020-01-28 18:21:20,902] ({main} Folder.java[addNote]:184) - Add note 2EXSDX8JZ to folder /
 INFO [2020-01-28 18:21:20,931] ({main} Folder.java[addNote]:184) - Add note 2EXSZT14S to folder /
 INFO [2020-01-28 18:21:20,965] ({main} Folder.java[addNote]:184) - Add note 2EXU94VGN to folder /
 INFO [2020-01-28 18:21:21,031] ({main} Folder.java[addNote]:184) - Add note 2EYJ17TMU to folder /
 INFO [2020-01-28 18:21:21,168] ({main} Folder.java[addNote]:184) - Add note 2EYSTPP46 to folder /
 INFO [2020-01-28 18:21:21,211] ({main} Folder.java[addNote]:184) - Add note 2EYYQ9E77 to folder /
 INFO [2020-01-28 18:21:21,234] ({main} Folder.java[addNote]:184) - Add note 2F1S8V93V to folder /
 INFO [2020-01-28 18:21:21,235] ({main} Notebook.java[<init>]:127) - Notebook indexing started...
 INFO [2020-01-28 18:21:21,594] ({main} LuceneSearch.java[addIndexDocs]:305) - Indexing 7 notebooks took 357ms
 INFO [2020-01-28 18:21:21,594] ({main} Notebook.java[<init>]:129) - Notebook indexing finished: 7 indexed in 0s
 INFO [2020-01-28 18:21:21,858] ({main} ServerImpl.java[initDestination]:94) - Setting the server's publish address to be /
 INFO [2020-01-28 18:21:23,154] ({main} ContextHandler.java[doStart]:744) - Started o.e.j.w.WebAppContext@2698dc7{/,file:/zeppelin/webapps/webapp/,AVAILABLE}{/zeppelin/zeppelin-web-0.7.2.war}
 INFO [2020-01-28 18:21:23,175] ({main} AbstractConnector.java[doStart]:266) - Started ServerConnector@6631cb64{HTTP/1.1}{0.0.0.0:8080}
 INFO [2020-01-28 18:21:23,176] ({main} Server.java[doStart]:379) - Started @14020ms
 INFO [2020-01-28 18:21:23,177] ({main} ZeppelinServer.java[main]:194) - Done, zeppelin server started
 WARN [2020-01-28 18:21:29,863] ({qtp1632392469-15} SecurityRestApi.java[ticket]:87) - {"status":"OK","message":"","body":{"principal":"anonymous","ticket":"anonymous","roles":"[]"}}
 INFO [2020-01-28 18:21:30,104] ({qtp1632392469-13} NotebookServer.java[onOpen]:156) - New connection from 172.21.0.1 : 54520
 INFO [2020-01-28 18:21:30,437] ({qtp1632392469-13} NotebookServer.java[sendNote]:705) - New operation from 172.21.0.1 : 54520 : anonymous : GET_NOTE : 2EYSTPP46
 WARN [2020-01-28 18:21:30,736] ({qtp1632392469-13} GitNotebookRepo.java[revisionHistory]:157) - No Head found for 2EYSTPP46, No HEAD exists and no explicit starting revision was specified
 WARN [2020-01-28 18:21:31,862] ({qtp1632392469-40} InterpreterSettingManager.java[getEditorSetting]:523) - Couldn't get interpreter editor setting
 WARN [2020-01-28 18:21:31,917] ({qtp1632392469-18} InterpreterSettingManager.java[getEditorSetting]:523) - Couldn't get interpreter editor setting
 WARN [2020-01-28 18:21:32,143] ({qtp1632392469-13} InterpreterSettingManager.java[getEditorSetting]:523) - Couldn't get interpreter editor setting
 WARN [2020-01-28 18:21:32,159] ({qtp1632392469-38} InterpreterSettingManager.java[getEditorSetting]:523) - Couldn't get interpreter editor setting
 WARN [2020-01-28 18:21:32,240] ({qtp1632392469-41} InterpreterSettingManager.java[getEditorSetting]:523) - Couldn't get interpreter editor setting
 INFO [2020-01-28 18:21:35,600] ({qtp1632392469-18} InterpreterFactory.java[createInterpretersForNote]:188) - Create interpreter instance spark for note 2EYSTPP46
 INFO [2020-01-28 18:21:35,615] ({qtp1632392469-18} InterpreterFactory.java[createInterpretersForNote]:221) - Interpreter org.apache.zeppelin.spark.SparkInterpreter 1696648649 created
 INFO [2020-01-28 18:21:35,616] ({qtp1632392469-18} InterpreterFactory.java[createInterpretersForNote]:221) - Interpreter org.apache.zeppelin.spark.SparkSqlInterpreter 2037435216 created
 INFO [2020-01-28 18:21:35,617] ({qtp1632392469-18} InterpreterFactory.java[createInterpretersForNote]:221) - Interpreter org.apache.zeppelin.spark.DepInterpreter 17563828 created
 INFO [2020-01-28 18:21:35,619] ({qtp1632392469-18} InterpreterFactory.java[createInterpretersForNote]:221) - Interpreter org.apache.zeppelin.spark.PySparkInterpreter 578547590 created
 INFO [2020-01-28 18:21:35,623] ({qtp1632392469-18} InterpreterFactory.java[createInterpretersForNote]:221) - Interpreter org.apache.zeppelin.spark.SparkRInterpreter 1096167557 created
 INFO [2020-01-28 18:21:41,073] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 18:21:41,082] ({pool-2-thread-2} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 18:21:41,120] ({pool-2-thread-2} RemoteInterpreterManagedProcess.java[start]:126) - Run interpreter process [/zeppelin/bin/interpreter.sh, -d, /zeppelin/interpreter/spark, -p, 39291, -l, /zeppelin/local-repo/2EZR2722K]
 INFO [2020-01-28 18:21:42,251] ({pool-2-thread-2} RemoteInterpreter.java[init]:221) - Create remote interpreter org.apache.zeppelin.spark.SparkInterpreter
 INFO [2020-01-28 18:21:42,509] ({pool-2-thread-2} RemoteInterpreter.java[pushAngularObjectRegistryToRemote]:551) - Push local angular object registry from ZeppelinServer to remote interpreter group 2EZR2722K:shared_process
 INFO [2020-01-28 18:21:42,545] ({pool-2-thread-2} RemoteInterpreter.java[init]:221) - Create remote interpreter org.apache.zeppelin.spark.SparkSqlInterpreter
 INFO [2020-01-28 18:21:42,558] ({pool-2-thread-2} RemoteInterpreter.java[init]:221) - Create remote interpreter org.apache.zeppelin.spark.DepInterpreter
 INFO [2020-01-28 18:21:42,596] ({pool-2-thread-2} RemoteInterpreter.java[init]:221) - Create remote interpreter org.apache.zeppelin.spark.PySparkInterpreter
 INFO [2020-01-28 18:21:42,641] ({pool-2-thread-2} RemoteInterpreter.java[init]:221) - Create remote interpreter org.apache.zeppelin.spark.SparkRInterpreter
 INFO [2020-01-28 18:21:43,206] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580220487673_589371948 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 18:21:43,208] ({pool-2-thread-3} Paragraph.java[jobRun]:362) - run paragraph 20200128-140807_1038512650 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 WARN [2020-01-28 18:21:51,119] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-140807_1038512650 is finished, status: ABORT, exception: null, result: null
 INFO [2020-01-28 18:21:52,121] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580220487673_589371948 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 18:22:03,639] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580220487673_589371948 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 18:22:03,641] ({pool-2-thread-3} Paragraph.java[jobRun]:362) - run paragraph 20200128-140807_1038512650 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 18:22:44,001] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-133627_1132513589 is finished successfully, status: FINISHED
 INFO [2020-01-28 18:22:44,674] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 18:23:25,354] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-140807_1038512650 is finished successfully, status: FINISHED
 INFO [2020-01-28 18:23:25,770] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580220487673_589371948 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 18:23:35,927] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218997687_-476942398 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 18:23:35,929] ({pool-2-thread-4} Paragraph.java[jobRun]:362) - run paragraph 20200128-134317_920707856 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 18:27:59,205] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-134317_920707856 is finished successfully, status: FINISHED
 INFO [2020-01-28 18:27:59,617] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218997687_-476942398 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 18:29:43,656] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580219638922_-945030212 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 18:29:43,657] ({pool-2-thread-2} Paragraph.java[jobRun]:362) - run paragraph 20200128-135358_1848431890 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 18:31:21,455] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-135358_1848431890 is finished successfully, status: FINISHED
 INFO [2020-01-28 18:31:21,708] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580219638922_-945030212 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 18:31:36,604] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580219452509_-1858868502 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 18:31:36,605] ({pool-2-thread-3} Paragraph.java[jobRun]:362) - run paragraph 20200128-135052_159782266 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 18:32:06,650] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-135052_159782266 is finished successfully, status: FINISHED
 INFO [2020-01-28 18:32:06,903] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580219452509_-1858868502 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 18:32:16,181] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580219499770_-1009114712 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 18:32:16,183] ({pool-2-thread-5} Paragraph.java[jobRun]:362) - run paragraph 20200128-135139_1663384573 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 18:32:18,290] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580219518041_2026969507 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 18:32:18,314] ({pool-2-thread-4} Paragraph.java[jobRun]:362) - run paragraph 20200128-135158_597330151 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 18:34:23,098] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-135139_1663384573 is finished successfully, status: FINISHED
 INFO [2020-01-28 18:34:23,435] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580219499770_-1009114712 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 18:34:40,940] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-135158_597330151 is finished successfully, status: FINISHED
 INFO [2020-01-28 18:34:41,145] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580219518041_2026969507 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 18:38:01,662] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580219544876_2052778331 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 18:38:01,666] ({pool-2-thread-6} Paragraph.java[jobRun]:362) - run paragraph 20200128-135224_1062116308 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 18:38:12,497] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-135224_1062116308 is finished successfully, status: FINISHED
 INFO [2020-01-28 18:38:12,748] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580219544876_2052778331 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:04:32,256] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580220487673_589371948 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:04:32,258] ({pool-2-thread-2} Paragraph.java[jobRun]:362) - run paragraph 20200128-140807_1038512650 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:04:33,858] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218997687_-476942398 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:04:33,862] ({pool-2-thread-7} Paragraph.java[jobRun]:362) - run paragraph 20200128-134317_920707856 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:05:08,175] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-140807_1038512650 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:05:08,673] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580220487673_589371948 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 WARN [2020-01-28 19:10:01,007] ({pool-2-thread-7} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-134317_920707856 is finished, status: ABORT, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 367, in <module>
    raise Exception(traceback.format_exc())
Exception: Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 360, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 7, in <module>
  File "/opt/conda/lib/python2.7/site-packages/ApplyKernel/__init__.py", line 42, in train
    self.trained=self.method.train( new_data[0] ,**kwargs)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/mllib/classification.py", line 553, in train
    return _regression_train_wrapper(train, SVMModel, data, initialWeights)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/mllib/regression.py", line 208, in _regression_train_wrapper
    first = data.first()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1361, in first
    rs = self.take(1)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1343, in take
    res = self.context.runJob(self, takeUpToNumLeft, p)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py", line 965, in runJob
    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1131, in __call__
    answer = self.gateway_client.send_command(command)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 883, in send_command
    response = connection.send_command(command)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1028, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/opt/conda/lib/python2.7/socket.py", line 451, in readline
    data = self._sock.recv(self._rbufsize)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py", line 236, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt


 WARN [2020-01-28 19:10:01,560] ({pool-2-thread-7} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-134317_920707856 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 367, in <module>
    raise Exception(traceback.format_exc())
Exception: Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 360, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 7, in <module>
  File "/opt/conda/lib/python2.7/site-packages/ApplyKernel/__init__.py", line 42, in train
    self.trained=self.method.train( new_data[0] ,**kwargs)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/mllib/classification.py", line 553, in train
    return _regression_train_wrapper(train, SVMModel, data, initialWeights)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/mllib/regression.py", line 208, in _regression_train_wrapper
    first = data.first()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1361, in first
    rs = self.take(1)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1343, in take
    res = self.context.runJob(self, takeUpToNumLeft, p)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py", line 965, in runJob
    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1131, in __call__
    answer = self.gateway_client.send_command(command)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 883, in send_command
    response = connection.send_command(command)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1028, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
  File "/opt/conda/lib/python2.7/socket.py", line 451, in readline
    data = self._sock.recv(self._rbufsize)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py", line 236, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt


 INFO [2020-01-28 19:10:02,024] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218997687_-476942398 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 WARN [2020-01-28 19:10:31,448] ({qtp1632392469-18} SecurityRestApi.java[ticket]:87) - {"status":"OK","message":"","body":{"principal":"anonymous","ticket":"anonymous","roles":"[]"}}
 INFO [2020-01-28 19:10:31,505] ({qtp1632392469-38} NotebookServer.java[onOpen]:156) - New connection from 172.21.0.1 : 55160
 INFO [2020-01-28 19:10:33,959] ({qtp1632392469-18} NotebookServer.java[sendNote]:705) - New operation from 172.21.0.1 : 55160 : anonymous : GET_NOTE : 2EYYQ9E77
 WARN [2020-01-28 19:10:33,997] ({qtp1632392469-18} GitNotebookRepo.java[revisionHistory]:157) - No Head found for 2EYYQ9E77, No HEAD exists and no explicit starting revision was specified
 WARN [2020-01-28 19:10:35,072] ({qtp1632392469-41} InterpreterSettingManager.java[getEditorSetting]:523) - Couldn't get interpreter editor setting
 WARN [2020-01-28 19:10:35,073] ({qtp1632392469-41} InterpreterSettingManager.java[getEditorSetting]:523) - Couldn't get interpreter editor setting
 WARN [2020-01-28 19:10:35,074] ({qtp1632392469-41} InterpreterSettingManager.java[getEditorSetting]:523) - Couldn't get interpreter editor setting
 WARN [2020-01-28 19:10:35,075] ({qtp1632392469-41} InterpreterSettingManager.java[getEditorSetting]:523) - Couldn't get interpreter editor setting
 WARN [2020-01-28 19:10:35,077] ({qtp1632392469-41} InterpreterSettingManager.java[getEditorSetting]:523) - Couldn't get interpreter editor setting
 WARN [2020-01-28 19:10:35,078] ({qtp1632392469-41} InterpreterSettingManager.java[getEditorSetting]:523) - Couldn't get interpreter editor setting
 INFO [2020-01-28 19:10:59,568] ({qtp1632392469-38} NotebookServer.java[sendNote]:705) - New operation from 172.21.0.1 : 55160 : anonymous : GET_NOTE : 2EYYQ9E77
 WARN [2020-01-28 19:10:59,608] ({qtp1632392469-38} GitNotebookRepo.java[revisionHistory]:157) - No Head found for 2EYYQ9E77, No HEAD exists and no explicit starting revision was specified
 WARN [2020-01-28 19:11:00,537] ({qtp1632392469-18} InterpreterSettingManager.java[getEditorSetting]:523) - Couldn't get interpreter editor setting
 WARN [2020-01-28 19:11:00,539] ({qtp1632392469-18} InterpreterSettingManager.java[getEditorSetting]:523) - Couldn't get interpreter editor setting
 WARN [2020-01-28 19:11:00,541] ({qtp1632392469-18} InterpreterSettingManager.java[getEditorSetting]:523) - Couldn't get interpreter editor setting
 WARN [2020-01-28 19:11:00,542] ({qtp1632392469-18} InterpreterSettingManager.java[getEditorSetting]:523) - Couldn't get interpreter editor setting
 WARN [2020-01-28 19:11:00,544] ({qtp1632392469-18} InterpreterSettingManager.java[getEditorSetting]:523) - Couldn't get interpreter editor setting
 WARN [2020-01-28 19:11:00,545] ({qtp1632392469-18} InterpreterSettingManager.java[getEditorSetting]:523) - Couldn't get interpreter editor setting
 INFO [2020-01-28 19:13:12,943] ({qtp1632392469-18} NotebookServer.java[broadcastNewParagraph]:641) - Broadcasting paragraph on run call instead of note.
 INFO [2020-01-28 19:13:13,225] ({qtp1632392469-18} NotebookServer.java[broadcastNewParagraph]:641) - Broadcasting paragraph on run call instead of note.
 INFO [2020-01-28 19:13:41,611] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580238792959_94612369 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:13:41,612] ({pool-2-thread-3} Paragraph.java[jobRun]:362) - run paragraph 20200128-191312_949907848 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 WARN [2020-01-28 19:13:41,624] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-191312_949907848 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 344, in <module>
    code = compile('\n'.join(final_code), '<stdin>', 'exec', ast.PyCF_ONLY_AST, 1)
  File "<stdin>", line 1
    from pyspark.ml.tuning import ParamGridBuilder, CrossValidatorparamGrid = (ParamGridBuilder()
                                                                            ^
SyntaxError: invalid syntax

 INFO [2020-01-28 19:13:41,815] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580238792959_94612369 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:15:54,060] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580220487673_589371948 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:15:54,063] ({pool-2-thread-8} Paragraph.java[jobRun]:362) - run paragraph 20200128-140807_1038512650 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:15:56,875] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218997687_-476942398 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:15:56,885] ({pool-2-thread-9} Paragraph.java[jobRun]:362) - run paragraph 20200128-134317_920707856 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:16:26,828] ({pool-2-thread-8} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-140807_1038512650 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:16:27,125] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580220487673_589371948 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:18:00,732] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-134317_920707856 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:18:00,912] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218997687_-476942398 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:18:08,642] ({qtp1632392469-41} NotebookServer.java[broadcastNewParagraph]:641) - Broadcasting paragraph on run call instead of note.
 INFO [2020-01-28 19:18:10,833] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580239088475_-1837198178 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:18:10,846] ({pool-2-thread-5} Paragraph.java[jobRun]:362) - run paragraph 20200128-191808_678436891 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:18:10,863] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-191808_678436891 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:18:11,086] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580239088475_-1837198178 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:18:16,060] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580239088475_-1837198178 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:18:16,069] ({pool-2-thread-4} Paragraph.java[jobRun]:362) - run paragraph 20200128-191808_678436891 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 WARN [2020-01-28 19:18:16,078] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-191808_678436891 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 367, in <module>
    raise Exception(traceback.format_exc())
Exception: Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 360, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 1, in <module>
AttributeError: ApplyKernel instance has no attribute 'collect'


 INFO [2020-01-28 19:18:16,259] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580239088475_-1837198178 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:18:19,991] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580239088475_-1837198178 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:18:19,994] ({pool-2-thread-10} Paragraph.java[jobRun]:362) - run paragraph 20200128-191808_678436891 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:18:20,007] ({pool-2-thread-10} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-191808_678436891 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:18:20,172] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580239088475_-1837198178 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:18:31,958] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580219638922_-945030212 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:18:31,960] ({pool-2-thread-6} Paragraph.java[jobRun]:362) - run paragraph 20200128-135358_1848431890 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:20:13,229] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-135358_1848431890 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:20:13,409] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580219638922_-945030212 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:20:16,756] ({pool-2-thread-11} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580219452509_-1858868502 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:20:16,758] ({pool-2-thread-11} Paragraph.java[jobRun]:362) - run paragraph 20200128-135052_159782266 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:20:45,276] ({pool-2-thread-11} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-135052_159782266 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:20:45,459] ({pool-2-thread-11} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580219452509_-1858868502 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:20:52,948] ({pool-2-thread-12} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580219499770_-1009114712 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:20:52,950] ({pool-2-thread-12} Paragraph.java[jobRun]:362) - run paragraph 20200128-135139_1663384573 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:20:55,898] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580219518041_2026969507 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:20:55,922] ({pool-2-thread-2} Paragraph.java[jobRun]:362) - run paragraph 20200128-135158_597330151 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:23:03,286] ({pool-2-thread-12} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-135139_1663384573 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:23:03,531] ({pool-2-thread-12} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580219499770_-1009114712 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:23:03,557] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580219544876_2052778331 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:23:03,558] ({pool-2-thread-7} Paragraph.java[jobRun]:362) - run paragraph 20200128-135224_1062116308 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:23:21,708] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-135158_597330151 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:23:22,017] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580219518041_2026969507 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:23:30,328] ({pool-2-thread-7} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-135224_1062116308 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:23:30,507] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580219544876_2052778331 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:30:39,520] ({pool-2-thread-13} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:30:39,522] ({pool-2-thread-13} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 WARN [2020-01-28 19:30:39,737] ({pool-2-thread-13} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-133627_1132513589 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 367, in <module>
    raise Exception(traceback.format_exc())
Exception: Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 355, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 5, in <module>
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1343, in take
    res = self.context.runJob(self, takeUpToNumLeft, p)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py", line 965, in runJob
    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 520.0 failed 1 times, most recent failure: Lost task 0.0 in stage 520.0 (TID 49736, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not tuple

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)
	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not tuple

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more



 INFO [2020-01-28 19:30:39,904] ({pool-2-thread-13} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:31:28,234] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:31:28,236] ({pool-2-thread-3} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 WARN [2020-01-28 19:31:28,416] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-133627_1132513589 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 367, in <module>
    raise Exception(traceback.format_exc())
Exception: Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 355, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 5, in <module>
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1343, in take
    res = self.context.runJob(self, takeUpToNumLeft, p)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py", line 965, in runJob
    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 521.0 failed 1 times, most recent failure: Lost task 0.0 in stage 521.0 (TID 49737, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not tuple

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)
	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not tuple

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more



 INFO [2020-01-28 19:31:28,602] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:31:34,557] ({pool-2-thread-14} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:31:34,559] ({pool-2-thread-14} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 WARN [2020-01-28 19:31:34,737] ({pool-2-thread-14} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-133627_1132513589 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 367, in <module>
    raise Exception(traceback.format_exc())
Exception: Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 355, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 5, in <module>
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1343, in take
    res = self.context.runJob(self, takeUpToNumLeft, p)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py", line 965, in runJob
    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 522.0 failed 1 times, most recent failure: Lost task 0.0 in stage 522.0 (TID 49738, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not tuple

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)
	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not tuple

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more



 INFO [2020-01-28 19:31:34,902] ({pool-2-thread-14} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:32:12,515] ({pool-2-thread-15} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:32:12,517] ({pool-2-thread-15} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 WARN [2020-01-28 19:32:12,568] ({pool-2-thread-15} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-133627_1132513589 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 367, in <module>
    raise Exception(traceback.format_exc())
Exception: Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 355, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 5, in <module>
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1343, in take
    res = self.context.runJob(self, takeUpToNumLeft, p)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py", line 965, in runJob
    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 2439, in _jrdd
    self._jrdd_deserializer, profiler)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 2372, in _wrap_function
    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 2358, in _prepare_for_python_RDD
    pickled_command = ser.dumps(command)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 440, in dumps
    return cloudpickle.dumps(obj, 2)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/cloudpickle.py", line 667, in dumps
    cp.dump(obj)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/cloudpickle.py", line 107, in dump
    return Pickler.dump(self, obj)
  File "/opt/conda/lib/python2.7/pickle.py", line 224, in dump
    self.save(obj)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 568, in save_tuple
    save(element)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/cloudpickle.py", line 214, in save_function
    self.save_function_tuple(obj)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/cloudpickle.py", line 251, in save_function_tuple
    save((code, closure, base_globals))
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 554, in save_tuple
    save(element)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 639, in _batch_appends
    save(x)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/cloudpickle.py", line 214, in save_function
    self.save_function_tuple(obj)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/cloudpickle.py", line 251, in save_function_tuple
    save((code, closure, base_globals))
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 554, in save_tuple
    save(element)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 639, in _batch_appends
    save(x)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/cloudpickle.py", line 214, in save_function
    self.save_function_tuple(obj)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/cloudpickle.py", line 251, in save_function_tuple
    save((code, closure, base_globals))
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 554, in save_tuple
    save(element)
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/opt/conda/lib/python2.7/pickle.py", line 606, in save_list
    self._batch_appends(iter(obj))
  File "/opt/conda/lib/python2.7/pickle.py", line 642, in _batch_appends
    save(tmp[0])
  File "/opt/conda/lib/python2.7/pickle.py", line 286, in save
    f(self, obj) # Call unbound method with explicit self
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/cloudpickle.py", line 136, in save_unsupported
    raise pickle.PicklingError("Cannot pickle objects of type %s" % type(obj))
PicklingError: Cannot pickle objects of type <type 'generator'>


 INFO [2020-01-28 19:32:12,876] ({pool-2-thread-15} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:32:22,963] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:32:22,965] ({pool-2-thread-8} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 WARN [2020-01-28 19:32:23,187] ({pool-2-thread-8} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-133627_1132513589 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 367, in <module>
    raise Exception(traceback.format_exc())
Exception: Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 355, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 5, in <module>
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1343, in take
    res = self.context.runJob(self, takeUpToNumLeft, p)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py", line 965, in runJob
    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 523.0 failed 1 times, most recent failure: Lost task 0.0 in stage 523.0 (TID 49739, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not tuple

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)
	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not tuple

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more



 INFO [2020-01-28 19:32:23,360] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:32:52,441] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:32:52,443] ({pool-2-thread-9} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 WARN [2020-01-28 19:32:52,598] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-133627_1132513589 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 367, in <module>
    raise Exception(traceback.format_exc())
Exception: Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 355, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 5, in <module>
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1343, in take
    res = self.context.runJob(self, takeUpToNumLeft, p)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py", line 965, in runJob
    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 524.0 failed 1 times, most recent failure: Lost task 0.0 in stage 524.0 (TID 49740, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not tuple

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)
	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not tuple

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more



 INFO [2020-01-28 19:32:52,816] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:33:04,481] ({pool-2-thread-16} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:33:04,483] ({pool-2-thread-16} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:33:04,643] ({pool-2-thread-16} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-133627_1132513589 is finished successfully, status: FINISHED
 WARN [2020-01-28 19:33:04,810] ({pool-2-thread-16} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-133627_1132513589 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 367, in <module>
    raise Exception(traceback.format_exc())
Exception: Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 355, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 5, in <module>
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1343, in take
    res = self.context.runJob(self, takeUpToNumLeft, p)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py", line 965, in runJob
    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 525.0 failed 1 times, most recent failure: Lost task 0.0 in stage 525.0 (TID 49741, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not tuple

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)
	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not tuple

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more



 INFO [2020-01-28 19:33:05,022] ({pool-2-thread-16} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:33:44,476] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:33:44,478] ({pool-2-thread-5} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:33:56,530] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-133627_1132513589 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:33:56,735] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:34:00,871] ({pool-2-thread-32} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:34:00,872] ({pool-2-thread-32} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:34:12,737] ({pool-2-thread-32} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-133627_1132513589 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:34:12,942] ({pool-2-thread-32} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:36:08,577] ({pool-2-thread-4} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:36:08,579] ({pool-2-thread-4} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:36:08,750] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-133627_1132513589 is finished successfully, status: FINISHED
 WARN [2020-01-28 19:36:08,916] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-133627_1132513589 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 367, in <module>
    raise Exception(traceback.format_exc())
Exception: Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 355, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 5, in <module>
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1343, in take
    res = self.context.runJob(self, takeUpToNumLeft, p)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py", line 965, in runJob
    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 534.0 failed 1 times, most recent failure: Lost task 0.0 in stage 534.0 (TID 49770, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not tuple

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)
	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not tuple

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more



 INFO [2020-01-28 19:36:09,090] ({pool-2-thread-4} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:36:52,009] ({pool-2-thread-18} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:36:52,011] ({pool-2-thread-18} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 WARN [2020-01-28 19:36:52,190] ({pool-2-thread-18} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-133627_1132513589 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 367, in <module>
    raise Exception(traceback.format_exc())
Exception: Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 355, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 5, in <module>
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1343, in take
    res = self.context.runJob(self, takeUpToNumLeft, p)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py", line 965, in runJob
    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 535.0 failed 1 times, most recent failure: Lost task 0.0 in stage 535.0 (TID 49771, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not list

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)
	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
	at sun.reflect.GeneratedMethodAccessor175.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not list

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more



 INFO [2020-01-28 19:36:52,343] ({pool-2-thread-18} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:37:01,281] ({pool-2-thread-10} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:37:01,283] ({pool-2-thread-10} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:37:12,634] ({pool-2-thread-10} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-133627_1132513589 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:37:12,804] ({pool-2-thread-10} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:37:19,203] ({pool-2-thread-19} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:37:19,205] ({pool-2-thread-19} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 WARN [2020-01-28 19:37:19,362] ({pool-2-thread-19} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-133627_1132513589 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 367, in <module>
    raise Exception(traceback.format_exc())
Exception: Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 355, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 5, in <module>
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1343, in take
    res = self.context.runJob(self, takeUpToNumLeft, p)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py", line 965, in runJob
    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 540.0 failed 1 times, most recent failure: Lost task 0.0 in stage 540.0 (TID 49786, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not tuple

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)
	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
	at sun.reflect.GeneratedMethodAccessor175.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not tuple

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more



 INFO [2020-01-28 19:37:19,522] ({pool-2-thread-19} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:37:23,254] ({pool-2-thread-6} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:37:23,257] ({pool-2-thread-6} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:37:35,068] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-133627_1132513589 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:37:35,254] ({pool-2-thread-6} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:37:56,716] ({pool-2-thread-20} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:37:56,717] ({pool-2-thread-20} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 WARN [2020-01-28 19:37:56,863] ({pool-2-thread-20} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-133627_1132513589 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 367, in <module>
    raise Exception(traceback.format_exc())
Exception: Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 355, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 5, in <module>
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1343, in take
    res = self.context.runJob(self, takeUpToNumLeft, p)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py", line 965, in runJob
    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 545.0 failed 1 times, most recent failure: Lost task 0.0 in stage 545.0 (TID 49801, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not tuple

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)
	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
	at sun.reflect.GeneratedMethodAccessor175.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not tuple

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more



 INFO [2020-01-28 19:37:57,104] ({pool-2-thread-20} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:39:03,047] ({pool-2-thread-11} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:39:03,050] ({pool-2-thread-11} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 WARN [2020-01-28 19:39:03,223] ({pool-2-thread-11} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-133627_1132513589 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 367, in <module>
    raise Exception(traceback.format_exc())
Exception: Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 355, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 5, in <module>
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1343, in take
    res = self.context.runJob(self, takeUpToNumLeft, p)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py", line 965, in runJob
    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 546.0 failed 1 times, most recent failure: Lost task 0.0 in stage 546.0 (TID 49802, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not list

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)
	at org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)
	at org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)
	at sun.reflect.GeneratedMethodAccessor175.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1339, in takeUpToNumLeft
  File "<stdin>", line 4, in <lambda>
TypeError: list indices must be integers, not list

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more



 INFO [2020-01-28 19:39:03,402] ({pool-2-thread-11} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:40:00,947] ({pool-2-thread-21} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:40:00,949] ({pool-2-thread-21} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:40:12,928] ({pool-2-thread-21} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-133627_1132513589 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:40:13,097] ({pool-2-thread-21} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:40:18,335] ({pool-2-thread-22} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:40:18,336] ({pool-2-thread-22} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 WARN [2020-01-28 19:40:18,756] ({pool-2-thread-22} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-133627_1132513589 is finished, status: ERROR, exception: null, result: %text [u'9.5426', u'7.3412', u'5.3683', u'10.3907', u'8.823']

%text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 367, in <module>
    raise Exception(traceback.format_exc())
Exception: Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 355, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 7, in <module>
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/mllib/feature.py", line 252, in fit
    jmodel = callMLlibFunc("fitStandardScaler", self.withMean, self.withStd, dataset)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/mllib/common.py", line 130, in callMLlibFunc
    return callJavaFunc(sc, api, *args)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/mllib/common.py", line 123, in callJavaFunc
    return _java2py(sc, func(*args))
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 319, in get_return_value
    format(target_id, ".", name), value)
Py4JJavaError: An error occurred while calling o2261.fitStandardScaler.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 552.0 failed 1 times, most recent failure: Lost task 5.0 in stage 552.0 (TID 49823, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/mllib/linalg/__init__.py", line 80, in _convert_to_vector
    raise TypeError("Cannot convert type %s into Vector" % type(l))
TypeError: Cannot convert type <type 'unicode'> into Vector

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1981)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)
	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1127)
	at org.apache.spark.mllib.feature.StandardScaler.fit(StandardScaler.scala:57)
	at org.apache.spark.mllib.api.python.PythonMLLibAPI.fitStandardScaler(PythonMLLibAPI.scala:628)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 174, in main
    process()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/worker.py", line 169, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/serializers.py", line 268, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/mllib/linalg/__init__.py", line 80, in _convert_to_vector
    raise TypeError("Cannot convert type %s into Vector" % type(l))
TypeError: Cannot convert type <type 'unicode'> into Vector

	at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)
	at org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)
	at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more



 INFO [2020-01-28 19:40:18,913] ({pool-2-thread-22} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:40:52,684] ({pool-2-thread-12} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:40:52,688] ({pool-2-thread-12} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:40:59,468] ({pool-2-thread-12} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-133627_1132513589 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:40:59,652] ({pool-2-thread-12} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:41:02,828] ({pool-2-thread-23} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580220487673_589371948 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:41:02,830] ({pool-2-thread-23} Paragraph.java[jobRun]:362) - run paragraph 20200128-140807_1038512650 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:41:22,256] ({pool-2-thread-23} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-140807_1038512650 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:41:22,433] ({pool-2-thread-23} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580220487673_589371948 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:41:25,164] ({pool-2-thread-2} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218997687_-476942398 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:41:25,165] ({pool-2-thread-2} Paragraph.java[jobRun]:362) - run paragraph 20200128-134317_920707856 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:41:31,449] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-134317_920707856 is finished successfully, status: FINISHED
 WARN [2020-01-28 19:41:31,640] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-134317_920707856 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 367, in <module>
    raise Exception(traceback.format_exc())
Exception: Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 360, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 8, in <module>
  File "/opt/conda/lib/python2.7/site-packages/ApplyKernel/__init__.py", line 42, in train
    self.trained=self.method.train( new_data[0] ,**kwargs)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/mllib/regression.py", line 431, in train
    return _regression_train_wrapper(train, LassoModel, data, initialWeights)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/mllib/regression.py", line 208, in _regression_train_wrapper
    first = data.first()
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1361, in first
    rs = self.take(1)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/rdd.py", line 1343, in take
    res = self.context.runJob(self, takeUpToNumLeft, p)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py", line 965, in runJob
    port = self._jvm.PythonRDD.runJob(self._jsc.sc(), mappedRDD._jrdd, partitions)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1133, in __call__
    answer, self.gateway_client, self.target_id, self.name)
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/sql/utils.py", line 67, in deco
    e.java_exception.getStackTrace()))
  File "/opt/conda/lib/python2.7/_abcoll.py", line 605, in __iter__
    v = self[i]
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_collections.py", line 191, in __getitem__
    return self.__compute_item(key)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_collections.py", line 174, in __compute_item
    return get_return_value(answer, self._gateway_client)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/protocol.py", line 333, in get_return_value
    return OUTPUT_CONVERTER[type](answer[2:], gateway_client)
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 2305, in <lambda>
    lambda target_id, gateway_client: JavaObject(target_id, gateway_client))
  File "/zeppelin/interpreter/spark/pyspark/py4j-0.10.4-src.zip/py4j/java_gateway.py", line 1168, in __init__
    lambda wr, cc=self._gateway_client, id=self._target_id:
  File "/zeppelin/interpreter/spark/pyspark/pyspark.zip/pyspark/context.py", line 236, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt


 INFO [2020-01-28 19:41:31,809] ({pool-2-thread-2} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218997687_-476942398 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:41:35,878] ({pool-2-thread-7} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218997687_-476942398 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:41:35,880] ({pool-2-thread-7} Paragraph.java[jobRun]:362) - run paragraph 20200128-134317_920707856 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:45:05,697] ({pool-2-thread-7} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-134317_920707856 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:45:05,864] ({pool-2-thread-7} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218997687_-476942398 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:45:21,332] ({pool-2-thread-24} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580219638922_-945030212 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:45:21,334] ({pool-2-thread-24} Paragraph.java[jobRun]:362) - run paragraph 20200128-135358_1848431890 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:45:23,409] ({pool-2-thread-13} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580219452509_-1858868502 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:45:23,415] ({pool-2-thread-13} Paragraph.java[jobRun]:362) - run paragraph 20200128-135052_159782266 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:46:50,334] ({pool-2-thread-24} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-135358_1848431890 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:46:50,565] ({pool-2-thread-24} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580219638922_-945030212 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:46:50,577] ({pool-2-thread-25} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580219499770_-1009114712 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:46:50,586] ({pool-2-thread-25} Paragraph.java[jobRun]:362) - run paragraph 20200128-135139_1663384573 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:47:20,866] ({pool-2-thread-13} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-135052_159782266 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:47:21,215] ({pool-2-thread-13} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580219452509_-1858868502 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:47:21,739] ({pool-2-thread-3} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580219518041_2026969507 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:47:21,755] ({pool-2-thread-3} Paragraph.java[jobRun]:362) - run paragraph 20200128-135158_597330151 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:48:55,447] ({pool-2-thread-25} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-135139_1663384573 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:48:55,644] ({pool-2-thread-25} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580219499770_-1009114712 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:48:56,146] ({pool-2-thread-26} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580219544876_2052778331 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:48:56,149] ({pool-2-thread-26} Paragraph.java[jobRun]:362) - run paragraph 20200128-135224_1062116308 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:49:13,959] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-135158_597330151 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:49:14,142] ({pool-2-thread-3} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580219518041_2026969507 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:49:23,026] ({pool-2-thread-26} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-135224_1062116308 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:49:23,213] ({pool-2-thread-26} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580219544876_2052778331 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:49:44,759] ({qtp1632392469-38} NotebookServer.java[broadcastNewParagraph]:641) - Broadcasting paragraph on run call instead of note.
 INFO [2020-01-28 19:50:19,957] ({pool-2-thread-14} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580240984600_618766777 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:50:19,958] ({pool-2-thread-14} Paragraph.java[jobRun]:362) - run paragraph 20200128-194944_1137080105 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:50:20,033] ({pool-2-thread-14} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-194944_1137080105 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:50:20,205] ({pool-2-thread-14} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580240984600_618766777 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:50:31,980] ({pool-2-thread-27} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580240984600_618766777 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:50:31,981] ({pool-2-thread-27} Paragraph.java[jobRun]:362) - run paragraph 20200128-194944_1137080105 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:50:32,061] ({pool-2-thread-27} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-194944_1137080105 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:50:32,228] ({pool-2-thread-27} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580240984600_618766777 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:50:42,010] ({pool-2-thread-15} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580240984600_618766777 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:50:42,011] ({pool-2-thread-15} Paragraph.java[jobRun]:362) - run paragraph 20200128-194944_1137080105 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 WARN [2020-01-28 19:50:42,091] ({pool-2-thread-15} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-194944_1137080105 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 367, in <module>
    raise Exception(traceback.format_exc())
Exception: Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 360, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 1, in <module>
AttributeError: 'list' object has no attribute 'transpose'


 INFO [2020-01-28 19:50:42,245] ({pool-2-thread-15} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580240984600_618766777 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:50:53,708] ({pool-2-thread-28} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580240984600_618766777 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:50:53,710] ({pool-2-thread-28} Paragraph.java[jobRun]:362) - run paragraph 20200128-194944_1137080105 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 WARN [2020-01-28 19:50:53,718] ({pool-2-thread-28} NotebookServer.java[afterStatusChange]:2058) - Job 20200128-194944_1137080105 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 367, in <module>
    raise Exception(traceback.format_exc())
Exception: Traceback (most recent call last):
  File "/tmp/zeppelin_pyspark-3166949396391511424.py", line 360, in <module>
    exec(code, _zcUserQueryNameSpace)
  File "<stdin>", line 1, in <module>
AttributeError: 'module' object has no attribute 'Array'


 INFO [2020-01-28 19:50:53,876] ({pool-2-thread-28} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580240984600_618766777 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:51:01,551] ({pool-2-thread-8} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580240984600_618766777 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:51:01,553] ({pool-2-thread-8} Paragraph.java[jobRun]:362) - run paragraph 20200128-194944_1137080105 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:51:01,633] ({pool-2-thread-8} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-194944_1137080105 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:51:01,810] ({pool-2-thread-8} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580240984600_618766777 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:51:37,302] ({pool-2-thread-29} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580240984600_618766777 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:51:37,303] ({pool-2-thread-29} Paragraph.java[jobRun]:362) - run paragraph 20200128-194944_1137080105 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:51:37,901] ({pool-2-thread-29} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-194944_1137080105 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:51:38,081] ({pool-2-thread-29} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580240984600_618766777 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:51:43,478] ({pool-2-thread-9} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580240984600_618766777 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:51:43,479] ({pool-2-thread-9} Paragraph.java[jobRun]:362) - run paragraph 20200128-194944_1137080105 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:51:44,013] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-194944_1137080105 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:51:44,199] ({pool-2-thread-9} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580240984600_618766777 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:51:52,494] ({pool-2-thread-30} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580240984600_618766777 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:51:52,496] ({pool-2-thread-30} Paragraph.java[jobRun]:362) - run paragraph 20200128-194944_1137080105 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:51:53,122] ({pool-2-thread-30} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-194944_1137080105 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:51:53,301] ({pool-2-thread-30} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580240984600_618766777 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:52:55,041] ({pool-2-thread-16} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580218587274_-195643858 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:52:55,042] ({pool-2-thread-16} Paragraph.java[jobRun]:362) - run paragraph 20200128-133627_1132513589 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:52:57,906] ({pool-2-thread-31} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580240984600_618766777 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:52:57,945] ({pool-2-thread-31} Paragraph.java[jobRun]:362) - run paragraph 20200128-194944_1137080105 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:53:02,142] ({pool-2-thread-16} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-133627_1132513589 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:53:02,358] ({pool-2-thread-16} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580218587274_-195643858 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:53:02,817] ({pool-2-thread-31} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-194944_1137080105 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:53:03,004] ({pool-2-thread-31} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580240984600_618766777 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:53:28,376] ({pool-2-thread-5} SchedulerFactory.java[jobStarted]:131) - Job paragraph_1580240984600_618766777 started by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
 INFO [2020-01-28 19:53:28,377] ({pool-2-thread-5} Paragraph.java[jobRun]:362) - run paragraph 20200128-194944_1137080105 using pyspark org.apache.zeppelin.interpreter.LazyOpenInterpreter@227bef86
 INFO [2020-01-28 19:53:29,173] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2056) - Job 20200128-194944_1137080105 is finished successfully, status: FINISHED
 INFO [2020-01-28 19:53:29,361] ({pool-2-thread-5} SchedulerFactory.java[jobFinished]:137) - Job paragraph_1580240984600_618766777 finished by scheduler org.apache.zeppelin.interpreter.remote.RemoteInterpretershared_session2019788387
